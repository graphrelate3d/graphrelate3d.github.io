
<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
    <!-- Replace the content tag with appropriate information -->
    <meta name="description" content="DESCRIPTION META TAG">
    <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
    <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
    <meta property="og:url" content="URL OF THE WEBSITE"/>
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
    <meta property="og:image" content="static/image/your_banner_image.png"/>
    <meta property="og:image:width" content="1200"/>
    <meta property="og:image:height" content="630"/>


    <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
    <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
    <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
    <meta name="twitter:card" content="summary_large_image">
    <!-- Keywords for your paper to be indexed by-->
    <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
    <meta name="viewport" content="width=device-width, initial-scale=1">


    <title>graphrelate3d</title>
    <!-- <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> -->
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="static/css/bulma.min.css">
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="static/css/index.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
    <script defer src="static/js/fontawesome.all.min.js"></script>
    <script src="static/js/bulma-carousel.min.js"></script>
    <script src="static/js/bulma-slider.min.js"></script>
    <script src="static/js/index.js"></script>
</head>

<body>


<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <h1 class="title is-4 publication-title">GraphRelate3D: Context-Dependent 3D Object Detection <br>
                                                            with Inter-Object Relationship Graphs</h1>
                    <div class="is-size-5 publication-authors">
                        <span class="author-block"><a href="https://scholar.google.com/citations?user=Bcohc-oAAAAJ&hl=zh-CN"
                                target="_blank">Mingyu Liu</a><sup>1</sup><sup>†</sup>, </span>
                        <span class="author-block"><a href="https://scholar.google.com/citations?hl=zh-CN&user=dJGmJCEAAAAJ"
                                target="_blank">Ekim Yurtsever</a><sup>2</sup>, </span>
                        <span class="author-block"><a href="https://www.linkedin.com/in/marc-brede-5ba3911a6/"
                                target="_blank">Marc Brede</a><sup>1</sup>,</span>
                        <span class="author-block"><a href="https://www.linkedin.com/in/jun-meng-b329691b4/"
                                target="_blank">Jun Meng</a><sup>1</sup>,</span>
                        <span class="author-block"><a href="https://walzimmer.github.io/website/" 
                                target="_blank">Walter Zimmer</a><sup>1</sup>,</span>
                        <span class="author-block"><a href="https://scholar.google.com/citations?user=FPYXLpQAAAAJ&hl=en"
                                target="_blank">Xingcheng Zhou</a><sup>1</sup>,</span>
                        <span class="author-block"><a href="https://scholar.google.com/citations?hl=zh-CN&user=QEz7ItgAAAAJ"
                                target="_blank">Bare Luka Zagar</a><sup>1</sup>,</span>
                        <span class="author-block"><a href="https://www.ce.cit.tum.de/air/people/yuning-cui/" 
                                target="_blank">Yuning Cui</a><sup>1</sup>,</span>
                        <span class="author-block"><a
                                href="https://www.ce.cit.tum.de/air/people/prof-dr-ing-habil-alois-knoll"
                                target="_blank">Alois C. Knoll</a><sup>1</sup></span>
                    </div>

                    <div class="is-size-5 publication-authors">
                        <span class="author-block"><sup>1</sup>Technical University of Munich</span>
                        <span class="author-block"><sup>2</sup>The Ohio State University</span>
                        <span class="eql-cntrb"><small><br><sup>†</sup>Corresponding Author</small></span>
                    </div>

                    <div class="column has-text-centered">
                        <div class="publication-links">

                            <!-- Github link -->
                            <span class="link-block">
                                    <a href="https://github.com/MingyuLiu1/Object_Relation" target="_blank"
                                       class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fab fa-github"></i>
                                        </span>
                                        <span>Code</span>
                                    </a>
                                </span>

 
                            <!-- Github link -->
                           <span class="link-block">
                                  <a href="https://arxiv.org/pdf/00000.pdf" target="_blank"
                                     class="external-link button is-normal is-rounded is-dark">
                                  <span class="icon">
                                    <i class="ai ai-arxiv"></i>
                                  </span>
                                  <span>arXiv</span>
                                  </a>
                            </span>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>


<!-- Overview  -->
<section class="section hero is-light">
    <div class="container is-max-desktop">
        <div class="columns" style="text-align: center">


            <div class="column is-max-desktop">


                <h2 class="title is-3">Framework</h2>

                <div class="container is-max-desktop">
                    <img src="figures/pipeline.png" alt="infrastructure_sensors" />
                    <!-- <img src="static/images/Frame_work.jpg" alt="infrastructure_sensors" style="width: auto; height: auto; max-width: none; max-height: none;"> -->

                    <h2 class="subtitle has-text-justify">
                        Two-stage 3D object detection network extended with the proposed inter-object relation module (shown in yellow). 
                        We implement our module after a detector's RPN and RoI pooling modules. 
                        The object relation module consists of a Graph Generator and a GNN. 
                        First, the Graph Generator conducts inter-object relation graphs based on the centers of proposal boxes. 
                        After that, the GNN takes the proposal features and box information to calculate and iteratively update the node features. 
                        Ultimately, the node features of the same node obtained by different layers are concatenated, which are the input of the following detection head.
                    </h2>
                    </div>


                <div class="content">


                </div>
            </div>
        </div>
    </div>
</section>
<!-- End overview -->

<section class="section hero">
    <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <h2 class="title is-3">Abstract</h2>
                <div class="content has-text-justified">
                    <p>
                        Accurate and effective 3D object detection is critical 
                        for ensuring the driving safety of autonomous vehicles. 
                        Recently, state-of-the-art two-stage 3D object detectors 
                        have exhibited promising performance. However, these methods 
                        refine proposals individually, ignoring the rich contextual 
                        information in the object relationships between the neighbor proposals. 
                        In this study, we introduce an object relation module, 
                        consisting of a graph generator and a graph neural network (GNN), 
                        to learn the spatial information from certain patterns to improve 3D object detection. 
                        Specifically, we create an inter-object relationship graph 
                        based on proposals in a frame via the graph generator to connect 
                        each proposal with its neighbor proposals. Afterward, the GNN module 
                        extracts edge features from the generated graph and iteratively refines 
                        proposal features with the captured edge features. Ultimately, we leverage 
                        the refined features as input to the detection head to obtain detection results. 
                        Our approach improves upon the baseline PV-RCNN on the KITTI validation set 
                        for the car class across easy, moderate, and hard difficulty levels by 
                        0.82%, 0.74%, and 0.58%, respectively. Additionally, our method 
                        outperforms the baseline by more than 1% under the moderate 
                        and hard levels BEV AP on the test server. 
                    </p>
                </div>
            </div>
        </div>
    </div>
</section>

<!-- Sensor setup  -->
<!-- <section class="section hero is-light">
    <div class="container is-max-desktop">
        <div class="columns">
            <div class="column is-centered has-text-centered">
                <h2 class="title is-3">Visualization Results 1</h2>

                <div class="item video1">
                    &nbsp;
                    
                    <video poster="" autoplay controls muted loop height="100%">
                        <source src="static/videos/snow_output.mp4" type="video/mp4">
                    </video>
                    <h2 class="subtitle has-text-justify">

                    <p>Visualizations of WA-RM3D on unseen snow samples. <br>
                        <strong>Left: WA-RM3D    Right: Baseline Model <strong> </p>
                    </h2>
                </div>
                

            </div>
        </div>
    </div>
</section> -->
<!-- End sensor setup -->

<!-- <section class="section hero is-light">
    <div class="container is-max-desktop">
        <div class="columns">
            <div class="column is-centered has-text-centered">
                <h2 class="title is-3">Visualization Results 2</h2>

                <div class="item video1">
                    &nbsp;
                    
                    <video poster="" autoplay controls muted loop height="100%">
                        <source src="static/videos/0_output.mp4" type="video/mp4">
                    </video>
                    <h2 class="subtitle has-text-justify">

                        <p>Visualizations of WA-RM3D on TUMTraf-I test set Sequence 1. <br>
                        <strong>Left: WA-RM3D    Right: Baseline Model <strong> </p>
                    </h2>
                </div>
                

            </div>
        </div>
    </div>
</section> -->


<!-- <section class="section hero is-light">
    <div class="container is-max-desktop">
        <div class="columns">
            <div class="column is-centered has-text-centered">
                <h2 class="title is-3">Visualization Results 4</h2>

                <div class="item video1">
                    &nbsp;
                    
                    <video poster="" autoplay controls muted loop height="100%">
                        <source src="static/videos/2_output.mp4" type="video/mp4">
                    </video>
                    <h2 class="subtitle has-text-justify">

                        <p>Visualizations of WA-RM3D on TUMTraf-I test set Sequence 3. <br>
                        <strong>Left: WA-RM3D    Right: Baseline Model <strong> </p>
                    </h2>
                </div>
                

            </div>
        </div>
    </div>
</section> -->


<!-- <footer class="footer">
    <div class="container">
        <div class="columns is-centered">
            <div class="column is-8">
                <div class="content has-text-centered">
                    <p>
                        The <strong>TUMTraf-Synthetic</strong> Dataset is licensed under <a
                            href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA
                        4.0</a>.
                    </p>
                </div>
            </div>
        </div>
    </div>
</footer> -->

<!-- Statcounter tracking code -->

<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

<!-- End of Statcounter Code -->

</body>

</html>
